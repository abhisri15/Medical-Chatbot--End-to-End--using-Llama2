{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK!\n"
     ]
    }
   ],
   "source": [
    "print('OK!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=\"3fdb7357-cd42-4cc5-bbf3-1bd862bec155\"\n",
    "PINECONE_API_ENV=\"quickstart\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data=load_pdf(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_split(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of chunk:  7020\n"
     ]
    }
   ],
   "source": [
    "text_chunks = text_split(extracted_data)\n",
    "print('Length of chunk: ',len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_hugging_face_model():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\IIIT Bhubaneswar\\ML\\Medical Chatbot using Llama2\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "f:\\IIIT Bhubaneswar\\ML\\Medical Chatbot using Llama2\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello World\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "# Create an instance of Pinecone class\n",
    "pinecone_instance = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "\n",
    "# Specify the index name and host\n",
    "index_name = \"medical-chatbot\"\n",
    "host = \"https://medical-chatbot-w9g326t.svc.aped-4627-b74a.pinecone.io\"  # Replace with your actual Pinecone host\n",
    "\n",
    "# Create the index instance\n",
    "index = pinecone_instance.Index(index_name=index_name, host=host)\n",
    "\n",
    "# Assuming you have an embeddings model (HuggingFaceEmbeddings) and text_chunks ready\n",
    "# Convert the texts into embeddings using the correct method (e.g., embed_documents)\n",
    "vectors = embeddings.embed_documents([t.page_content for t in text_chunks])\n",
    "\n",
    "# Prepare the data for upserting\n",
    "# Each vector should be associated with a unique ID\n",
    "ids = [f\"id-{i}\" for i in range(len(vectors))]\n",
    "upsert_data = list(zip(ids, vectors))\n",
    "\n",
    "# Batch size for each upsert (ensure each batch is small enough to fit the 4MB limit)\n",
    "batch_size = 100  # Adjust batch size depending on the size of your vectors\n",
    "\n",
    "# Function to upsert in batches\n",
    "def upsert_in_batches(index, upsert_data, batch_size):\n",
    "    for i in range(0, len(upsert_data), batch_size):\n",
    "        batch = upsert_data[i:i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "# Upsert the vectors into the Pinecone index in batches\n",
    "upsert_in_batches(index, upsert_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(query_embedding))  # Should be a list or numpy array of floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "print(len(query_embedding))  # Length of query embedding\n",
    "# Assuming 'vectors' contains the document embeddings:\n",
    "print(len(vectors[0]))  # Length of the document embeddings in the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create a Pinecone client instance\n",
    "pinecone_client = pinecone.Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_API_ENV)\n",
    "\n",
    "# Index name\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "# Embeddings model (You can use Hugging Face model)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create index if it doesn't exist\n",
    "if index_name not in pinecone_client.list_indexes():\n",
    "    # Define the dimension of the embeddings\n",
    "    embedding_dim = len(embeddings.embed_query(\"test\"))\n",
    "    \n",
    "    # Create the index directly with the dimension and metric\n",
    "    pinecone_client.create_index(\n",
    "        name=index_name,\n",
    "        dimension=embedding_dim,  # The dimension of the embeddings\n",
    "        metric='cosine'           # Metric for similarity (cosine similarity)\n",
    "    )\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone_client.Index(index_name)\n",
    "\n",
    "# Assuming you have text_chunks available\n",
    "texts = [t.page_content for t in text_chunks]\n",
    "\n",
    "# Convert the texts into embeddings\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "\n",
    "# Generate unique IDs for the text chunks\n",
    "ids = [f\"id-{i}\" for i in range(len(vectors))]\n",
    "\n",
    "# Prepare upsert data\n",
    "upsert_data = list(zip(ids, vectors))\n",
    "\n",
    "# Batch size for upserting\n",
    "batch_size = 100\n",
    "\n",
    "# Function to upsert the vectors in batches\n",
    "def upsert_in_batches(index, upsert_data, batch_size):\n",
    "    for i in range(0, len(upsert_data), batch_size):\n",
    "        batch = upsert_data[i:i + batch_size]\n",
    "        index.upsert(vectors=batch)\n",
    "\n",
    "# Upsert the vectors into the Pinecone index\n",
    "upsert_in_batches(index, upsert_data, batch_size)\n",
    "\n",
    "# Now, let's perform a similarity search with a query\n",
    "query = 'What are allergies?'\n",
    "\n",
    "# Embed the query\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Perform the similarity search with Pinecone\n",
    "result = index.query(queries=[query_embedding], top_k=3)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template=\"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':512,\n",
    "                          'temperature':0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 2}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input=input(f\"Input Prompt:\")\n",
    "    result=qa({\"query\": user_input})\n",
    "    print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
